{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather Forecast: \n",
      "Monday: 25°C, Sunny\n",
      "Tuesday: 22°C, Cloudy\n",
      "Wednesday: 18°C, Rainy\n",
      "Thursday: 20°C, Partly Cloudy\n",
      "Friday: 30°C, Sunny\n",
      "Highest Temperature: Friday (30°C)\n",
      "Days with 'Sunny' condition: Monday, Friday\n",
      "Average temp for the week : 23.00°C\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Check if the weather.html file exists\n",
    "if os.path.exists('weather.html'):\n",
    "    with open('weather.html', 'r', encoding= 'utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    forecast = []\n",
    "    tablerows = soup.find_all('tr')[1:]\n",
    "    for row in tablerows:\n",
    "        cols = row.find_all('td')\n",
    "        day = cols[0].text\n",
    "        temp = int(cols[1].text.strip().replace('°C', ''))\n",
    "        condition = cols[2].text\n",
    "        forecast.append((day,temp,condition))\n",
    "    print(\"Weather Forecast: \")\n",
    "    for day,temp,condition in forecast:\n",
    "        print(f\"{day}: {temp}°C, {condition}\")\n",
    "    hight_temp = max(forecast, key=lambda x: x[1])\n",
    "    print(f\"Highest Temperature: {hight_temp[0]} ({hight_temp[1]}°C)\")\n",
    "    sunny_days = [day for day, temp, condition in forecast if condition == 'Sunny']\n",
    "    print(f\"Days with 'Sunny' condition: {', '.join(sunny_days)}\")\n",
    "\n",
    "    average_temp = sum(temp for day, temp, condition in forecast)/len(forecast)\n",
    "    print(f\"Average temp for the week : {average_temp:.2f}°C\")\n",
    "else:\n",
    "    print(\"weather.html file not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New job listings have been added or updated in the database.\n",
      "Filtered jobs (Location: Stewartbury, AA, Company: Payne, Roberts and Davis):\n",
      "('Senior Python Developer', 'Payne, Roberts and Davis', 'Stewartbury, AA', 'Professional asset web application environmentally friendly detail-oriented asset. Coordinate educational dashboard agile employ growth opportunity. Company programs CSS explore role. Html educational grit web application. Oversea SCRUM talented support. Web Application fast-growing communities inclusive programs job CSS. Css discussions growth opportunity explore open-minded oversee. Css Python environmentally friendly collaborate inclusive role. Django no experience oversee dashboard environmentally friendly willing to learn programs. Programs open-minded programs asset.', 'https://realpython.github.io/fake-jobs/jobs/senior-python-developer-0.html')\n",
      "Filtered jobs have been exported to filtered_jobs.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sqlite3\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Connect to SQLite database and create the table if it doesn't exist\n",
    "with sqlite3.connect(\"jobs.db\") as con:\n",
    "    cursor = con.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS jobs (\n",
    "            Job_Title TEXT,\n",
    "            Company_Name TEXT,\n",
    "            Location TEXT,\n",
    "            Job_Description TEXT,\n",
    "            Application_Link TEXT,\n",
    "            PRIMARY KEY (Job_Title, Company_Name, Location)\n",
    "        )\n",
    "    ''')\n",
    "\n",
    "def scrape_jobs():\n",
    "    url = \"https://realpython.github.io/fake-jobs\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    job_listing = []\n",
    "\n",
    "    for job_card in soup.find_all('div', class_='card-content'):\n",
    "        job_title = job_card.find('h2', class_='title is-5').text.strip() if job_card.find('h2', class_='title is-5') else 'No title'\n",
    "        company_name = job_card.find('h3', class_='subtitle is-6 company').text.strip() if job_card.find('h3', class_='subtitle is-6 company') else 'No company name'\n",
    "        location = job_card.find('p', class_='location').text.strip() if job_card.find('p', class_='location') else 'No location'\n",
    "        application_link = job_card.find('a', string='Apply').get('href').strip() if job_card.find('a', string='Apply') else None\n",
    "\n",
    "        if application_link:\n",
    "            response1 = requests.get(application_link)\n",
    "            if response1.status_code != 200:\n",
    "                print(f\"Failed to fetch job description for {job_title}\")\n",
    "                continue\n",
    "\n",
    "            soup1 = BeautifulSoup(response1.content, 'html.parser')\n",
    "            job_description = soup1.find('div', class_='content').find('p').text.strip() if soup1.find('div', class_='content') else 'No description'\n",
    "            job_listing.append((job_title, company_name, location, job_description, application_link))\n",
    "        \n",
    "        # Delay between requests to avoid overloading the server\n",
    "        time.sleep(1)\n",
    "\n",
    "    return job_listing\n",
    "\n",
    "def incremental_load(job_listing):\n",
    "    for job in job_listing:\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO jobs (Job_Title, Company_Name, Location, Job_Description, Application_Link)\n",
    "            VALUES (?, ?, ?, ?, ?)\n",
    "        ''', job)\n",
    "        con.commit()\n",
    "\n",
    "def filter_jobs(location=None, company_name=None):\n",
    "    query = \"SELECT * FROM jobs WHERE 1=1\"\n",
    "    params = []\n",
    "\n",
    "    if location:\n",
    "        query += \" AND Location LIKE ?\"\n",
    "        params.append(f\"%{location}%\")\n",
    "    \n",
    "    if company_name:\n",
    "        query += \" AND Company_Name LIKE ?\"\n",
    "        params.append(f\"%{company_name}%\")\n",
    "\n",
    "    cursor.execute(query, params)\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def export_to_csv(filtered_jobs, filename=\"filtered_jobs.csv\"):\n",
    "    with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Job Title\", \"Company Name\", \"Location\", \"Job Description\", \"Application Link\"])\n",
    "        writer.writerows(filtered_jobs)\n",
    "    print(f\"Filtered jobs have been exported to {filename}.\")\n",
    "\n",
    "def main():\n",
    "    job_listing = scrape_jobs()\n",
    "    if job_listing:\n",
    "        incremental_load(job_listing)\n",
    "        print(\"New job listings have been added or updated in the database.\")\n",
    "\n",
    "    # Example: Filter jobs by location or company name\n",
    "    location_filter = \"Stewartbury, AA\"  # Example location\n",
    "    company_filter = \"Payne, Roberts and Davis\"  # Example company name\n",
    "\n",
    "    filtered_jobs = filter_jobs(location=location_filter, company_name=company_filter)\n",
    "    print(f\"Filtered jobs (Location: {location_filter}, Company: {company_filter}):\")\n",
    "    for job in filtered_jobs:\n",
    "        print(job)\n",
    "\n",
    "    # Export filtered results to CSV\n",
    "    export_to_csv(filtered_jobs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visiting laptop page...\n",
      "Next page button clicked...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import json\n",
    "import time\n",
    "# Brauzer driverini ishga tushirish\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    # Saytga o'tish\n",
    "    driver.get(\"https://www.demoblaze.com/\")\n",
    "    \n",
    "    # Laptops tugmasini topish va bosish\n",
    "    buttons = driver.find_elements(By.ID, \"itemc\")\n",
    "    for button in buttons:\n",
    "        if \"byCat('notebook')\" in button.get_attribute('onclick'):\n",
    "            button.click()\n",
    "            print(\"Visiting laptop page...\")\n",
    "            break\n",
    "\n",
    "    # Sahifaning to'liq yuklanishini kutish\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.ID, \"next2\"))\n",
    "    )\n",
    "\n",
    "    # Next tugmasini bosish\n",
    "    next_button = driver.find_element(By.ID, \"next2\")\n",
    "    next_button.click()\n",
    "    print(\"Next page button clicked...\")\n",
    "\n",
    "    time.sleep(10)\n",
    "    laptops_data = []\n",
    "    products = driver.find_elements(By.CLASS_NAME, 'col-lg-4.col-md-6.mb-4')\n",
    "    for product in products:\n",
    "        name = product.find_element(By.CLASS_NAME, 'card-title').text\n",
    "        price = product.find_element(By.TAG_NAME, 'h5').text\n",
    "        description = product.find_element(By.CLASS_NAME, 'card-text').text\n",
    "\n",
    "        laptops_data.append(\n",
    "            {\n",
    "            \"name\": name,\n",
    "            \"price\": price,\n",
    "            \"description\": description\n",
    "            }\n",
    "            )\n",
    "    with open(\"laptops_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(laptops_data, f, indent=4)\n",
    "\n",
    "finally:\n",
    "    # Brauzerni yopish\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
